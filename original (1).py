# -*- coding: utf-8 -*-
"""original.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GZwA3oz05Hu0wd1hmUatnhLrzsuqw__4
"""

import torch # We no longer import as tch
import torch.nn as nn
import numpy as np
import torchvision # Contains data sets and functions for image processing
import torchvision.transforms as transforms # Contains MNIST, other image datasets, and image processing functions
import matplotlib.pyplot as plt
from time import time as tm
import torch.nn.functional as F
from copy import deepcopy
import seaborn as sns

# Import TorchSeq2PC
!git clone https://github.com/RobertRosenbaum/Torch2PC.git
from Torch2PC import TorchSeq2PC as T2PC

SaveFigures=False

if SaveFigures:
  from google.colab import drive
  drive.mount("/content/gdrive/")


def ToOneHot(labels, num_classes):
    y = torch.eye(num_classes)
    return y[labels]

torch.manual_seed(0)


# # This patches an error that sometimes arises in
# # downloading MNIST
# from six.moves import urllib
# opener = urllib.request.build_opener()
# opener.addheaders = [('User-agent', 'Mozilla/5.0')]
# urllib.request.install_opener(opener)

# # This can also help errors that arise sometimes
# # when downloading MNIST
!wget -nc www.di.ens.fr/~lelarge/MNIST.tar.gz
!tar -zxvf MNIST.tar.gz

# Load training and testing data from MNIST dataset
# These lines return data structures that contain
# the training and testing data
from torchvision.datasets import MNIST

train_dataset = MNIST('./',
      train=True,
      transform=transforms.ToTensor(),
      download=True)

test_dataset = MNIST('./',
      train=False,
      transform=transforms.ToTensor(),
      download=True)


#train_dataset.data=train_dataset.data/255.0
#test_dataset.data=test_dataset.data/255.0

# Print the size of the two data sets
m = len(train_dataset)
mtest = len(test_dataset)
print("Number of data points in training set = ",m)
print("Number of data points in test set=",mtest)

# train_dataset.data contains all the MNIST images (X)
# train_dataset.targets contains all the labels (Y)
print("Size of training inputs (X)=",train_dataset.data.size())
print("Size of training labels (Y)=",train_dataset.targets.size())

batch_size = 300      # Batch size to use with training data
test_batch_size = 300 # Batch size to use for test data

# Data loader. These make it easy to iterate through batches of data.
# Shuffle=True means that the data will be randomly shuffled on every epoch
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                          batch_size=batch_size,
                                          shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=test_batch_size,
                                          shuffle=True)


steps_per_epoch = len(train_loader) # = mini batch size = m'
print("steps per epoch (mini batch size)=",steps_per_epoch)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('device = ',device)

model=nn.Sequential(

    nn.Sequential(nn.Conv2d(1,10,3),
    nn.ReLU(),
    nn.MaxPool2d(2)
    ),

    nn.Sequential(
    nn.Conv2d(10,5,3),
    nn.ReLU(),
    nn.Flatten()
    ),

 nn.Sequential(
    nn.Linear(5*11*11,50),
    nn.ReLU()
    ),

 nn.Sequential(
    nn.Linear(50,30),
    nn.ReLU()
    ),


nn.Sequential(
   nn.Linear(30,10)
 )

).to(device)

LossFun = nn.MSELoss()

# Compute one output and one loss to make sure
# things are working
with torch.no_grad():
  TrainingIterator=iter(train_loader)
  X,Y=next(TrainingIterator)
  X=X.to(device)
  Y=ToOneHot(Y,10).to(device)
  Yhat=model(X).to(device)
  print('output shape = ',Yhat.shape)
  print('loss on initial model = ',LossFun(Yhat,Y).item())

NumParams=sum(p.numel() for p in model.parameters() if p.requires_grad)
print('Number of trainable parameters in model =',NumParams)


LearningRate=.002
num_epochs=2
ComputeTrainingMetrics=True
WhichOptimizer=torch.optim.Adam
PrintEvery=50
total_num_steps  = num_epochs*steps_per_epoch

torch.manual_seed(0)

modelPC=deepcopy(model)


optimizerPC = WhichOptimizer(modelPC.parameters(), lr=LearningRate)


jj=0
LossesToPlotPC=np.zeros(total_num_steps)
TestLossesToPlotPC=np.zeros(total_num_steps)
AccuraciesToPlotPC=np.zeros(total_num_steps)
TestAccuraciesToPlotPC=np.zeros(total_num_steps)

GradsRelDiff0=np.zeros([total_num_steps,len(model)])
GradsCosSim0=np.zeros([total_num_steps,len(model)])
GradsAngle0=np.zeros([total_num_steps,len(model)])

def RelDiff(x,y):
  return np.linalg.norm(x-y)/np.linalg.norm(y)

def corr2(x,y):
  c=np.corrcoef(x,y)
  return c[0,1]

CosSim = nn.CosineSimilarity(dim=0, eps=1e-8)


eta=.1
n=20

j=0     # Counter to keep track of iterations
t1=tm() # Start the timer

for k in range(num_epochs):

  # Re-initializes the training iterator (shuffles data for one epoch)
  TrainingIterator=iter(train_loader)

  for i in range(steps_per_epoch): # For each batch

    # Get one batch of training data, reshape it
    # and send it to the current device
    X,Y=next(TrainingIterator)
    X=X.to(device)
    Y=ToOneHot(Y,10).to(device)

    _,Loss,_,_,_=T2PC.PCInfer(modelPC,LossFun,X,Y,"Strict",eta,n)

    if ComputeTrainingMetrics:
      modelBP=deepcopy(modelPC)   # Copy the model
      YhatBP = modelBP(X)         # Forward pass
      LossBP = LossFun(YhatBP, Y)
      LossBP.backward()       # Compute gradients
      for layer in range(len(model)):
        gradsPC=modelPC[layer][0].weight.grad.cpu().detach().numpy()
        gradsBP=modelBP[layer][0].weight.grad.cpu().detach().numpy()
        GradsRelDiff0[jj,layer]=RelDiff(gradsPC,gradsBP)
        GradsCosSim0[jj,layer]=CosSim(torch.tensor(gradsPC.flatten()),torch.tensor(gradsBP.flatten())).item()
        GradsAngle0[jj,layer]=torch.acos(torch.tensor(GradsCosSim0[jj,layer])).item()
      modelBP.zero_grad()

    # Update parameters
    optimizerPC.step()


    # Zero-out gradients
    modelPC.zero_grad()
    optimizerPC.zero_grad()

    # Print loss, store loss, compute test loss
    with torch.no_grad():
      if(i%PrintEvery==0):
        print('k =',k,'i =',i,'L =',Loss.item())
      LossesToPlotPC[jj]=Loss.item()

      if ComputeTrainingMetrics:
        Yhat=modelPC(X)
        AccuraciesToPlotPC[jj]=(torch.sum(torch.argmax(Y,axis=1)==torch.argmax(Yhat,axis=1))/test_batch_size).item()
        modelPC.eval()
        TestingIterator=iter(test_loader)
        Xtest,Ytest=next(TestingIterator)
        Xtest=Xtest.to(device)
        Ytest=ToOneHot(Ytest,10).to(device)
        YhatTest=modelPC(Xtest)
        TestLossesToPlotPC[jj]=LossFun(YhatTest,Ytest).item()
        TestAccuraciesToPlotPC[jj]=(torch.sum(torch.argmax(Ytest,axis=1)==torch.argmax(YhatTest,axis=1))/test_batch_size).item()
        modelPC.train()
      jj+=1




tTrain=tm()-t1
print('Training time = ',tTrain,'sec')

plt.figure()
plt.plot(LossesToPlotPC)
plt.ylim(bottom=0)
plt.ylabel('training loss')

plt.figure()
plt.plot(GradsRelDiff0)
plt.ylabel('rel err of dtheta')

torch.manual_seed(0)

modelBP=deepcopy(model)

optimizerBP = WhichOptimizer(modelBP.parameters(), lr=LearningRate)

total_num_steps  = num_epochs*steps_per_epoch


jj=0
LossesToPlotBP=np.zeros(total_num_steps)
TestLossesToPlotBP=np.zeros(total_num_steps)
AccuraciesToPlotBP=np.zeros(total_num_steps)
TestAccuraciesToPlotBP=np.zeros(total_num_steps)


DepthPlusOne=len(modelBP)+1 # Number of layers, counting the input as layer 0

j=0     # Counter to keep track of iterations
t1=tm() # Start the timer
for k in range(num_epochs):

  # Re-initializes the training iterator (shuffles data for one epoch)
  TrainingIterator=iter(train_loader)

  for i in range(steps_per_epoch): # For each batch

    # Get one batch of training data, reshape it
    # and send it to the current device
    X,Y=next(TrainingIterator)
    X=X.to(device)
    Y=ToOneHot(Y,10).to(device)


    Yhat = modelBP(X)     # Forward pass
    Loss = LossFun(Yhat, Y)
    Loss.backward()       # Compute gradients
    optimizerBP.step()      # Update parameters


    # Zero-out gradients
    modelBP.zero_grad()
    optimizerBP.zero_grad()

    with torch.no_grad():
      if(i%PrintEvery==0):
        print('k =',k,'i =',i,'L =',Loss.item())
      LossesToPlotBP[jj]=Loss.item()
      if ComputeTrainingMetrics:
        AccuraciesToPlotBP[jj]=(torch.sum(torch.argmax(Y,axis=1)==torch.argmax(Yhat,axis=1))/test_batch_size).item()
        modelBP.eval()
        TestingIterator=iter(test_loader)
        Xtest,Ytest=next(TestingIterator)
        Xtest=Xtest.to(device)
        Ytest=ToOneHot(Ytest,10).to(device)
        YhatTest=modelBP(Xtest)
        TestLossesToPlotBP[jj]=LossFun(YhatTest,Ytest).item()
        TestAccuraciesToPlotBP[jj]=(torch.sum(torch.argmax(Ytest,axis=1)==torch.argmax(YhatTest,axis=1))/test_batch_size).item()
        modelBP.train()
      jj+=1

tTrainBP=tm()-t1
print('Training time = ',tTrainBP,'sec')

plt.figure()
plt.plot(LossesToPlotBP)
plt.ylim(bottom=0)
plt.ylabel('training loss')

sns.set(context='notebook',style='white',font_scale=1.3)

fig, axes = plt.subplots(figsize=(17, 3))


plt.subplot(1,4,1)
plt.plot(LossesToPlotPC,color=sns.color_palette('pastel')[3],label='Modified PC (train)')
plt.plot(LossesToPlotBP,color=sns.color_palette('pastel')[0],label='BP (train)')
plt.plot(TestLossesToPlotPC,color=sns.color_palette('dark')[3],label='Modified PC (test)')
plt.plot(TestLossesToPlotBP,color=sns.color_palette('dark')[0],label='BP (test)')
plt.xlabel('step number')
plt.ylabel('MNIST\n\n loss')
sns.despine()
plt.legend()
plt.ylim(bottom=0)
plt.title('A',loc='left')


plt.subplot(1,4,2)
plt.plot(AccuraciesToPlotPC,color=sns.color_palette('pastel')[3],label='Modified PC (train)')
plt.plot(AccuraciesToPlotBP,color=sns.color_palette('pastel')[0],label='BP (train)')
plt.plot(TestAccuraciesToPlotPC,color=sns.color_palette('dark')[3],label='Modified PC (test)')
plt.plot(TestAccuraciesToPlotBP,color=sns.color_palette('dark')[0],label='BP (test)')
plt.xlabel('step number')
plt.ylabel('accuracy')
sns.despine()
#plt.legend()
plt.ylim(bottom=0)
plt.title('B',loc='left')

plt.subplot(1,4,3)
for layer in range(len(model)):
  plt.plot(GradsRelDiff0[:,layer],label='layer '+str(layer+1))
#plt.plot(GradsRelDiff)
plt.xlabel('step number')
plt.ylabel(r'relative error of d$\theta$'+'\nfrom gradient')
plt.legend(loc='center')
sns.despine()
plt.title('C',loc='left')

plt.subplot(1,4,4)
plt.plot((180/3.14159)*GradsAngle0)
plt.xlabel('step number')
plt.ylabel(r'angle between d$\theta$ and'+'\n gradient (degrees)')
sns.despine()
plt.title('D',loc='left')

plt.tight_layout()

if SaveFigures:
  plt.savefig('/content/gdrive/MyDrive/PredictiveCodingANN/Figures/PCMNIST.pdf')

"""Exp 1"""

batch_size = 300      # Batch size to use with training data
test_batch_size = 300 # Batch size to use for test data

# Data loader. These make it easy to iterate through batches of data.
# Shuffle=True means that the data will be randomly shuffled on every epoch
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                          batch_size=batch_size,
                                          shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=test_batch_size,
                                          shuffle=True)


steps_per_epoch = len(train_loader) # = mini batch size = m'
print("steps per epoch (mini batch size)=",steps_per_epoch)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('device = ',device)

model=nn.Sequential(

    nn.Sequential(nn.Conv2d(1,10,3),
    nn.ReLU(),
    nn.MaxPool2d(2)
    ),

    nn.Sequential(
    nn.Conv2d(10,5,3),
    nn.ReLU(),
    nn.Flatten()
    ),

 nn.Sequential(
    nn.Linear(5*11*11,50),
    nn.ReLU()
    ),

 nn.Sequential(
    nn.Linear(50,30),
    nn.ReLU()
    ),


nn.Sequential(
   nn.Linear(30,10)
 )

).to(device)

LossFun = nn.MSELoss()

# Compute one output and one loss to make sure
# things are working
with torch.no_grad():
  TrainingIterator=iter(train_loader)
  X,Y=next(TrainingIterator)
  X=X.to(device)
  Y=ToOneHot(Y,10).to(device)
  Yhat=model(X).to(device)
  print('output shape = ',Yhat.shape)
  print('loss on initial model = ',LossFun(Yhat,Y).item())

NumParams=sum(p.numel() for p in model.parameters() if p.requires_grad)
print('Number of trainable parameters in model =',NumParams)


LearningRate=0.01
num_epochs=1
ComputeTrainingMetrics=True
WhichOptimizer=torch.optim.Adam
PrintEvery=50
total_num_steps  = num_epochs*steps_per_epoch

torch.manual_seed(0)

modelPC=deepcopy(model)


optimizerPC = WhichOptimizer(modelPC.parameters(), lr=LearningRate)


jj=0
LossesToPlotPC=np.zeros(total_num_steps)
TestLossesToPlotPC=np.zeros(total_num_steps)
AccuraciesToPlotPC=np.zeros(total_num_steps)
TestAccuraciesToPlotPC=np.zeros(total_num_steps)

GradsRelDiff0=np.zeros([total_num_steps,len(model)])
GradsCosSim0=np.zeros([total_num_steps,len(model)])
GradsAngle0=np.zeros([total_num_steps,len(model)])

def RelDiff(x,y):
  return np.linalg.norm(x-y)/np.linalg.norm(y)

def corr2(x,y):
  c=np.corrcoef(x,y)
  return c[0,1]

CosSim = nn.CosineSimilarity(dim=0, eps=1e-8)


eta=.1
n=20

j=0     # Counter to keep track of iterations
t1=tm() # Start the timer

for k in range(num_epochs):

  # Re-initializes the training iterator (shuffles data for one epoch)
  TrainingIterator=iter(train_loader)

  for i in range(steps_per_epoch): # For each batch

    # Get one batch of training data, reshape it
    # and send it to the current device
    X,Y=next(TrainingIterator)
    X=X.to(device)
    Y=ToOneHot(Y,10).to(device)

    _,Loss,_,_,_=T2PC.PCInfer(modelPC,LossFun,X,Y,"Strict",eta,n)

    if ComputeTrainingMetrics:
      modelBP=deepcopy(modelPC)   # Copy the model
      YhatBP = modelBP(X)         # Forward pass
      LossBP = LossFun(YhatBP, Y)
      LossBP.backward()       # Compute gradients
      for layer in range(len(model)):
        gradsPC=modelPC[layer][0].weight.grad.cpu().detach().numpy()
        gradsBP=modelBP[layer][0].weight.grad.cpu().detach().numpy()
        GradsRelDiff0[jj,layer]=RelDiff(gradsPC,gradsBP)
        GradsCosSim0[jj,layer]=CosSim(torch.tensor(gradsPC.flatten()),torch.tensor(gradsBP.flatten())).item()
        GradsAngle0[jj,layer]=torch.acos(torch.tensor(GradsCosSim0[jj,layer])).item()
      modelBP.zero_grad()

    # Update parameters
    optimizerPC.step()


    # Zero-out gradients
    modelPC.zero_grad()
    optimizerPC.zero_grad()

    # Print loss, store loss, compute test loss
    with torch.no_grad():
      if(i%PrintEvery==0):
        print('k =',k,'i =',i,'L =',Loss.item())
      LossesToPlotPC[jj]=Loss.item()

      if ComputeTrainingMetrics:
        Yhat=modelPC(X)
        AccuraciesToPlotPC[jj]=(torch.sum(torch.argmax(Y,axis=1)==torch.argmax(Yhat,axis=1))/test_batch_size).item()
        modelPC.eval()
        TestingIterator=iter(test_loader)
        Xtest,Ytest=next(TestingIterator)
        Xtest=Xtest.to(device)
        Ytest=ToOneHot(Ytest,10).to(device)
        YhatTest=modelPC(Xtest)
        TestLossesToPlotPC[jj]=LossFun(YhatTest,Ytest).item()
        TestAccuraciesToPlotPC[jj]=(torch.sum(torch.argmax(Ytest,axis=1)==torch.argmax(YhatTest,axis=1))/test_batch_size).item()
        modelPC.train()
      jj+=1




tTrain=tm()-t1
print('Training time = ',tTrain,'sec')

plt.figure()
plt.plot(LossesToPlotPC)
plt.ylim(bottom=0)
plt.ylabel('training loss')

plt.figure()
plt.plot(GradsRelDiff0)
plt.ylabel('rel err of dtheta')

torch.manual_seed(0)

modelBP=deepcopy(model)

optimizerBP = WhichOptimizer(modelBP.parameters(), lr=LearningRate)

total_num_steps  = num_epochs*steps_per_epoch


jj=0
LossesToPlotBP=np.zeros(total_num_steps)
TestLossesToPlotBP=np.zeros(total_num_steps)
AccuraciesToPlotBP=np.zeros(total_num_steps)
TestAccuraciesToPlotBP=np.zeros(total_num_steps)


DepthPlusOne=len(modelBP)+1 # Number of layers, counting the input as layer 0

j=0     # Counter to keep track of iterations
t1=tm() # Start the timer
for k in range(num_epochs):

  # Re-initializes the training iterator (shuffles data for one epoch)
  TrainingIterator=iter(train_loader)

  for i in range(steps_per_epoch): # For each batch

    # Get one batch of training data, reshape it
    # and send it to the current device
    X,Y=next(TrainingIterator)
    X=X.to(device)
    Y=ToOneHot(Y,10).to(device)


    Yhat = modelBP(X)     # Forward pass
    Loss = LossFun(Yhat, Y)
    Loss.backward()       # Compute gradients
    optimizerBP.step()      # Update parameters


    # Zero-out gradients
    modelBP.zero_grad()
    optimizerBP.zero_grad()

    with torch.no_grad():
      if(i%PrintEvery==0):
        print('k =',k,'i =',i,'L =',Loss.item())
      LossesToPlotBP[jj]=Loss.item()
      if ComputeTrainingMetrics:
        AccuraciesToPlotBP[jj]=(torch.sum(torch.argmax(Y,axis=1)==torch.argmax(Yhat,axis=1))/test_batch_size).item()
        modelBP.eval()
        TestingIterator=iter(test_loader)
        Xtest,Ytest=next(TestingIterator)
        Xtest=Xtest.to(device)
        Ytest=ToOneHot(Ytest,10).to(device)
        YhatTest=modelBP(Xtest)
        TestLossesToPlotBP[jj]=LossFun(YhatTest,Ytest).item()
        TestAccuraciesToPlotBP[jj]=(torch.sum(torch.argmax(Ytest,axis=1)==torch.argmax(YhatTest,axis=1))/test_batch_size).item()
        modelBP.train()
      jj+=1

tTrainBP=tm()-t1
print('Training time = ',tTrainBP,'sec')

plt.figure()
plt.plot(LossesToPlotBP)
plt.ylim(bottom=0)
plt.ylabel('training loss')

sns.set(context='notebook',style='white',font_scale=1.3)

fig, axes = plt.subplots(figsize=(17, 3))


plt.subplot(1,4,1)
plt.plot(LossesToPlotPC,color=sns.color_palette('pastel')[3],label='Modified PC (train)')
plt.plot(LossesToPlotBP,color=sns.color_palette('pastel')[0],label='BP (train)')
plt.plot(TestLossesToPlotPC,color=sns.color_palette('dark')[3],label='Modified PC (test)')
plt.plot(TestLossesToPlotBP,color=sns.color_palette('dark')[0],label='BP (test)')
plt.xlabel('step number')
plt.ylabel('MNIST\n\n loss')
sns.despine()
plt.legend()
plt.ylim(bottom=0)
plt.title('A',loc='left')


plt.subplot(1,4,2)
plt.plot(AccuraciesToPlotPC,color=sns.color_palette('pastel')[3],label='Modified PC (train)')
plt.plot(AccuraciesToPlotBP,color=sns.color_palette('pastel')[0],label='BP (train)')
plt.plot(TestAccuraciesToPlotPC,color=sns.color_palette('dark')[3],label='Modified PC (test)')
plt.plot(TestAccuraciesToPlotBP,color=sns.color_palette('dark')[0],label='BP (test)')
plt.xlabel('step number')
plt.ylabel('accuracy')
sns.despine()
#plt.legend()
plt.ylim(bottom=0)
plt.title('B',loc='left')

plt.subplot(1,4,3)
for layer in range(len(model)):
  plt.plot(GradsRelDiff0[:,layer],label='layer '+str(layer+1))
#plt.plot(GradsRelDiff)
plt.xlabel('step number')
plt.ylabel(r'relative error of d$\theta$'+'\nfrom gradient')
plt.legend(loc='center')
sns.despine()
plt.title('C',loc='left')

plt.subplot(1,4,4)
plt.plot((180/3.14159)*GradsAngle0)
plt.xlabel('step number')
plt.ylabel(r'angle between d$\theta$ and'+'\n gradient (degrees)')
sns.despine()
plt.title('D',loc='left')

plt.tight_layout()

if SaveFigures:
  plt.savefig('/content/gdrive/MyDrive/PredictiveCodingANN/Figures/PCMNIST.pdf')

"""exp 4"""

batch_size = 300      # Batch size to use with training data
test_batch_size = 300 # Batch size to use for test data

# Data loader. These make it easy to iterate through batches of data.
# Shuffle=True means that the data will be randomly shuffled on every epoch
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                          batch_size=batch_size,
                                          shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=test_batch_size,
                                          shuffle=True)


steps_per_epoch = len(train_loader) # = mini batch size = m'
print("steps per epoch (mini batch size)=",steps_per_epoch)

import torch
import torch.nn as nn

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('device =', device)

model = nn.Sequential(
    nn.Sequential(nn.Conv2d(1, 10, 3),
                  nn.Sigmoid(),  # Change activation function to sigmoid
                  nn.MaxPool2d(2)
                  ),

    nn.Sequential(nn.Conv2d(10, 5, 3),
                  nn.Sigmoid(),  # Change activation function to sigmoid
                  nn.Flatten()
                  ),

    nn.Sequential(nn.Linear(5 * 11 * 11, 50),
                  nn.Sigmoid(),  # Change activation function to sigmoid
                  ),

    nn.Sequential(nn.Linear(50, 30),
                  nn.Sigmoid(),  # Change activation function to sigmoid
                  ),

    nn.Sequential(nn.Linear(30, 10)
                  )
).to(device)

LossFun = nn.MSELoss()

# Compute one output and one loss to make sure things are working
with torch.no_grad():
    TrainingIterator = iter(train_loader)
    X, Y = next(TrainingIterator)
    X = X.to(device)
    Y = ToOneHot(Y, 10).to(device)
    Yhat = model(X).to(device)
    print('output shape =', Yhat.shape)
    print('loss on initial model =', LossFun(Yhat, Y).item())

NumParams = sum(p.numel() for p in model.parameters() if p.requires_grad)
print('Number of trainable parameters in model =', NumParams)

LearningRate = 0.002
num_epochs = 2
ComputeTrainingMetrics = True
WhichOptimizer = torch.optim.Adam
PrintEvery = 50
total_num_steps = num_epochs * steps_per_epoch

torch.manual_seed(0)

modelPC=deepcopy(model)


optimizerPC = WhichOptimizer(modelPC.parameters(), lr=LearningRate)


jj=0
LossesToPlotPC=np.zeros(total_num_steps)
TestLossesToPlotPC=np.zeros(total_num_steps)
AccuraciesToPlotPC=np.zeros(total_num_steps)
TestAccuraciesToPlotPC=np.zeros(total_num_steps)

GradsRelDiff0=np.zeros([total_num_steps,len(model)])
GradsCosSim0=np.zeros([total_num_steps,len(model)])
GradsAngle0=np.zeros([total_num_steps,len(model)])

def RelDiff(x,y):
  return np.linalg.norm(x-y)/np.linalg.norm(y)

def corr2(x,y):
  c=np.corrcoef(x,y)
  return c[0,1]

CosSim = nn.CosineSimilarity(dim=0, eps=1e-8)


eta=.1
n=20

j=0     # Counter to keep track of iterations
t1=tm() # Start the timer

for k in range(num_epochs):

  # Re-initializes the training iterator (shuffles data for one epoch)
  TrainingIterator=iter(train_loader)

  for i in range(steps_per_epoch): # For each batch

    # Get one batch of training data, reshape it
    # and send it to the current device
    X,Y=next(TrainingIterator)
    X=X.to(device)
    Y=ToOneHot(Y,10).to(device)

    _,Loss,_,_,_=T2PC.PCInfer(modelPC,LossFun,X,Y,"Strict",eta,n)

    if ComputeTrainingMetrics:
      modelBP=deepcopy(modelPC)   # Copy the model
      YhatBP = modelBP(X)         # Forward pass
      LossBP = LossFun(YhatBP, Y)
      LossBP.backward()       # Compute gradients
      for layer in range(len(model)):
        gradsPC=modelPC[layer][0].weight.grad.cpu().detach().numpy()
        gradsBP=modelBP[layer][0].weight.grad.cpu().detach().numpy()
        GradsRelDiff0[jj,layer]=RelDiff(gradsPC,gradsBP)
        GradsCosSim0[jj,layer]=CosSim(torch.tensor(gradsPC.flatten()),torch.tensor(gradsBP.flatten())).item()
        GradsAngle0[jj,layer]=torch.acos(torch.tensor(GradsCosSim0[jj,layer])).item()
      modelBP.zero_grad()

    # Update parameters
    optimizerPC.step()


    # Zero-out gradients
    modelPC.zero_grad()
    optimizerPC.zero_grad()

    # Print loss, store loss, compute test loss
    with torch.no_grad():
      if(i%PrintEvery==0):
        print('k =',k,'i =',i,'L =',Loss.item())
      LossesToPlotPC[jj]=Loss.item()

      if ComputeTrainingMetrics:
        Yhat=modelPC(X)
        AccuraciesToPlotPC[jj]=(torch.sum(torch.argmax(Y,axis=1)==torch.argmax(Yhat,axis=1))/test_batch_size).item()
        modelPC.eval()
        TestingIterator=iter(test_loader)
        Xtest,Ytest=next(TestingIterator)
        Xtest=Xtest.to(device)
        Ytest=ToOneHot(Ytest,10).to(device)
        YhatTest=modelPC(Xtest)
        TestLossesToPlotPC[jj]=LossFun(YhatTest,Ytest).item()
        TestAccuraciesToPlotPC[jj]=(torch.sum(torch.argmax(Ytest,axis=1)==torch.argmax(YhatTest,axis=1))/test_batch_size).item()
        modelPC.train()
      jj+=1




tTrain=tm()-t1
print('Training time = ',tTrain,'sec')

plt.figure()
plt.plot(LossesToPlotPC)
plt.ylim(bottom=0)
plt.ylabel('training loss')

plt.figure()
plt.plot(GradsRelDiff0)
plt.ylabel('rel err of dtheta')

torch.manual_seed(0)

modelBP=deepcopy(model)

optimizerBP = WhichOptimizer(modelBP.parameters(), lr=LearningRate)

total_num_steps  = num_epochs*steps_per_epoch


jj=0
LossesToPlotBP=np.zeros(total_num_steps)
TestLossesToPlotBP=np.zeros(total_num_steps)
AccuraciesToPlotBP=np.zeros(total_num_steps)
TestAccuraciesToPlotBP=np.zeros(total_num_steps)


DepthPlusOne=len(modelBP)+1 # Number of layers, counting the input as layer 0

j=0     # Counter to keep track of iterations
t1=tm() # Start the timer
for k in range(num_epochs):

  # Re-initializes the training iterator (shuffles data for one epoch)
  TrainingIterator=iter(train_loader)

  for i in range(steps_per_epoch): # For each batch

    # Get one batch of training data, reshape it
    # and send it to the current device
    X,Y=next(TrainingIterator)
    X=X.to(device)
    Y=ToOneHot(Y,10).to(device)


    Yhat = modelBP(X)     # Forward pass
    Loss = LossFun(Yhat, Y)
    Loss.backward()       # Compute gradients
    optimizerBP.step()      # Update parameters


    # Zero-out gradients
    modelBP.zero_grad()
    optimizerBP.zero_grad()

    with torch.no_grad():
      if(i%PrintEvery==0):
        print('k =',k,'i =',i,'L =',Loss.item())
      LossesToPlotBP[jj]=Loss.item()
      if ComputeTrainingMetrics:
        AccuraciesToPlotBP[jj]=(torch.sum(torch.argmax(Y,axis=1)==torch.argmax(Yhat,axis=1))/test_batch_size).item()
        modelBP.eval()
        TestingIterator=iter(test_loader)
        Xtest,Ytest=next(TestingIterator)
        Xtest=Xtest.to(device)
        Ytest=ToOneHot(Ytest,10).to(device)
        YhatTest=modelBP(Xtest)
        TestLossesToPlotBP[jj]=LossFun(YhatTest,Ytest).item()
        TestAccuraciesToPlotBP[jj]=(torch.sum(torch.argmax(Ytest,axis=1)==torch.argmax(YhatTest,axis=1))/test_batch_size).item()
        modelBP.train()
      jj+=1

tTrainBP=tm()-t1
print('Training time = ',tTrainBP,'sec')

plt.figure()
plt.plot(LossesToPlotBP)
plt.ylim(bottom=0)
plt.ylabel('training loss')

sns.set(context='notebook',style='white',font_scale=1.3)

fig, axes = plt.subplots(figsize=(17, 3))


plt.subplot(1,4,1)
plt.plot(LossesToPlotPC,color=sns.color_palette('pastel')[3],label='Modified PC (train)')
plt.plot(LossesToPlotBP,color=sns.color_palette('pastel')[0],label='BP (train)')
plt.plot(TestLossesToPlotPC,color=sns.color_palette('dark')[3],label='Modified PC (test)')
plt.plot(TestLossesToPlotBP,color=sns.color_palette('dark')[0],label='BP (test)')
plt.xlabel('step number')
plt.ylabel('MNIST\n\n loss')
sns.despine()
plt.legend()
plt.ylim(bottom=0)
plt.title('A',loc='left')


plt.subplot(1,4,2)
plt.plot(AccuraciesToPlotPC,color=sns.color_palette('pastel')[3],label='Modified PC (train)')
plt.plot(AccuraciesToPlotBP,color=sns.color_palette('pastel')[0],label='BP (train)')
plt.plot(TestAccuraciesToPlotPC,color=sns.color_palette('dark')[3],label='Modified PC (test)')
plt.plot(TestAccuraciesToPlotBP,color=sns.color_palette('dark')[0],label='BP (test)')
plt.xlabel('step number')
plt.ylabel('accuracy')
sns.despine()
#plt.legend()
plt.ylim(bottom=0)
plt.title('B',loc='left')

plt.subplot(1,4,3)
for layer in range(len(model)):
  plt.plot(GradsRelDiff0[:,layer],label='layer '+str(layer+1))
#plt.plot(GradsRelDiff)
plt.xlabel('step number')
plt.ylabel(r'relative error of d$\theta$'+'\nfrom gradient')
plt.legend(loc='center')
sns.despine()
plt.title('C',loc='left')

plt.subplot(1,4,4)
plt.plot((180/3.14159)*GradsAngle0)
plt.xlabel('step number')
plt.ylabel(r'angle between d$\theta$ and'+'\n gradient (degrees)')
sns.despine()
plt.title('D',loc='left')

plt.tight_layout()

if SaveFigures:
  plt.savefig('/content/gdrive/MyDrive/PredictiveCodingANN/Figures/PCMNIST.pdf')

"""Exp 3"""

batch_size = 500      # Batch size to use with training data
test_batch_size = 500 # Batch size to use for test data

# Data loader. These make it easy to iterate through batches of data.
# Shuffle=True means that the data will be randomly shuffled on every epoch
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                          batch_size=batch_size,
                                          shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=test_batch_size,
                                          shuffle=True)


steps_per_epoch = len(train_loader) # = mini batch size = m'
print("steps per epoch (mini batch size)=",steps_per_epoch)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('device = ',device)

model=nn.Sequential(

    nn.Sequential(nn.Conv2d(1,10,3),
    nn.ReLU(),
    nn.MaxPool2d(2)
    ),

    nn.Sequential(
    nn.Conv2d(10,5,3),
    nn.ReLU(),
    nn.Flatten()
    ),

 nn.Sequential(
    nn.Linear(5*11*11,50),
    nn.ReLU()
    ),

 nn.Sequential(
    nn.Linear(50,30),
    nn.ReLU()
    ),


nn.Sequential(
   nn.Linear(30,10)
 )

).to(device)

LossFun = nn.MSELoss()

# Compute one output and one loss to make sure
# things are working
with torch.no_grad():
  TrainingIterator=iter(train_loader)
  X,Y=next(TrainingIterator)
  X=X.to(device)
  Y=ToOneHot(Y,10).to(device)
  Yhat=model(X).to(device)
  print('output shape = ',Yhat.shape)
  print('loss on initial model = ',LossFun(Yhat,Y).item())

NumParams=sum(p.numel() for p in model.parameters() if p.requires_grad)
print('Number of trainable parameters in model =',NumParams)


LearningRate=.002
num_epochs=2
ComputeTrainingMetrics=True
WhichOptimizer=torch.optim.Adam
PrintEvery=50
total_num_steps  = num_epochs*steps_per_epoch

torch.manual_seed(0)

modelPC=deepcopy(model)


optimizerPC = WhichOptimizer(modelPC.parameters(), lr=LearningRate)


jj=0
LossesToPlotPC=np.zeros(total_num_steps)
TestLossesToPlotPC=np.zeros(total_num_steps)
AccuraciesToPlotPC=np.zeros(total_num_steps)
TestAccuraciesToPlotPC=np.zeros(total_num_steps)

GradsRelDiff0=np.zeros([total_num_steps,len(model)])
GradsCosSim0=np.zeros([total_num_steps,len(model)])
GradsAngle0=np.zeros([total_num_steps,len(model)])

def RelDiff(x,y):
  return np.linalg.norm(x-y)/np.linalg.norm(y)

def corr2(x,y):
  c=np.corrcoef(x,y)
  return c[0,1]

CosSim = nn.CosineSimilarity(dim=0, eps=1e-8)


eta=.1
n=20

j=0     # Counter to keep track of iterations
t1=tm() # Start the timer

for k in range(num_epochs):

  # Re-initializes the training iterator (shuffles data for one epoch)
  TrainingIterator=iter(train_loader)

  for i in range(steps_per_epoch): # For each batch

    # Get one batch of training data, reshape it
    # and send it to the current device
    X,Y=next(TrainingIterator)
    X=X.to(device)
    Y=ToOneHot(Y,10).to(device)

    _,Loss,_,_,_=T2PC.PCInfer(modelPC,LossFun,X,Y,"Strict",eta,n)

    if ComputeTrainingMetrics:
      modelBP=deepcopy(modelPC)   # Copy the model
      YhatBP = modelBP(X)         # Forward pass
      LossBP = LossFun(YhatBP, Y)
      LossBP.backward()       # Compute gradients
      for layer in range(len(model)):
        gradsPC=modelPC[layer][0].weight.grad.cpu().detach().numpy()
        gradsBP=modelBP[layer][0].weight.grad.cpu().detach().numpy()
        GradsRelDiff0[jj,layer]=RelDiff(gradsPC,gradsBP)
        GradsCosSim0[jj,layer]=CosSim(torch.tensor(gradsPC.flatten()),torch.tensor(gradsBP.flatten())).item()
        GradsAngle0[jj,layer]=torch.acos(torch.tensor(GradsCosSim0[jj,layer])).item()
      modelBP.zero_grad()

    # Update parameters
    optimizerPC.step()


    # Zero-out gradients
    modelPC.zero_grad()
    optimizerPC.zero_grad()

    # Print loss, store loss, compute test loss
    with torch.no_grad():
      if(i%PrintEvery==0):
        print('k =',k,'i =',i,'L =',Loss.item())
      LossesToPlotPC[jj]=Loss.item()

      if ComputeTrainingMetrics:
        Yhat=modelPC(X)
        AccuraciesToPlotPC[jj]=(torch.sum(torch.argmax(Y,axis=1)==torch.argmax(Yhat,axis=1))/test_batch_size).item()
        modelPC.eval()
        TestingIterator=iter(test_loader)
        Xtest,Ytest=next(TestingIterator)
        Xtest=Xtest.to(device)
        Ytest=ToOneHot(Ytest,10).to(device)
        YhatTest=modelPC(Xtest)
        TestLossesToPlotPC[jj]=LossFun(YhatTest,Ytest).item()
        TestAccuraciesToPlotPC[jj]=(torch.sum(torch.argmax(Ytest,axis=1)==torch.argmax(YhatTest,axis=1))/test_batch_size).item()
        modelPC.train()
      jj+=1




tTrain=tm()-t1
print('Training time = ',tTrain,'sec')

plt.figure()
plt.plot(LossesToPlotPC)
plt.ylim(bottom=0)
plt.ylabel('training loss')

plt.figure()
plt.plot(GradsRelDiff0)
plt.ylabel('rel err of dtheta')

torch.manual_seed(0)

modelBP=deepcopy(model)

optimizerBP = WhichOptimizer(modelBP.parameters(), lr=LearningRate)

total_num_steps  = num_epochs*steps_per_epoch


jj=0
LossesToPlotBP=np.zeros(total_num_steps)
TestLossesToPlotBP=np.zeros(total_num_steps)
AccuraciesToPlotBP=np.zeros(total_num_steps)
TestAccuraciesToPlotBP=np.zeros(total_num_steps)


DepthPlusOne=len(modelBP)+1 # Number of layers, counting the input as layer 0

j=0     # Counter to keep track of iterations
t1=tm() # Start the timer
for k in range(num_epochs):

  # Re-initializes the training iterator (shuffles data for one epoch)
  TrainingIterator=iter(train_loader)

  for i in range(steps_per_epoch): # For each batch

    # Get one batch of training data, reshape it
    # and send it to the current device
    X,Y=next(TrainingIterator)
    X=X.to(device)
    Y=ToOneHot(Y,10).to(device)


    Yhat = modelBP(X)     # Forward pass
    Loss = LossFun(Yhat, Y)
    Loss.backward()       # Compute gradients
    optimizerBP.step()      # Update parameters


    # Zero-out gradients
    modelBP.zero_grad()
    optimizerBP.zero_grad()

    with torch.no_grad():
      if(i%PrintEvery==0):
        print('k =',k,'i =',i,'L =',Loss.item())
      LossesToPlotBP[jj]=Loss.item()
      if ComputeTrainingMetrics:
        AccuraciesToPlotBP[jj]=(torch.sum(torch.argmax(Y,axis=1)==torch.argmax(Yhat,axis=1))/test_batch_size).item()
        modelBP.eval()
        TestingIterator=iter(test_loader)
        Xtest,Ytest=next(TestingIterator)
        Xtest=Xtest.to(device)
        Ytest=ToOneHot(Ytest,10).to(device)
        YhatTest=modelBP(Xtest)
        TestLossesToPlotBP[jj]=LossFun(YhatTest,Ytest).item()
        TestAccuraciesToPlotBP[jj]=(torch.sum(torch.argmax(Ytest,axis=1)==torch.argmax(YhatTest,axis=1))/test_batch_size).item()
        modelBP.train()
      jj+=1

tTrainBP=tm()-t1
print('Training time = ',tTrainBP,'sec')

plt.figure()
plt.plot(LossesToPlotBP)
plt.ylim(bottom=0)
plt.ylabel('training loss')

sns.set(context='notebook',style='white',font_scale=1.3)

fig, axes = plt.subplots(figsize=(17, 3))


plt.subplot(1,4,1)
plt.plot(LossesToPlotPC,color=sns.color_palette('pastel')[3],label='Modified PC (train)')
plt.plot(LossesToPlotBP,color=sns.color_palette('pastel')[0],label='BP (train)')
plt.plot(TestLossesToPlotPC,color=sns.color_palette('dark')[3],label='Modified PC (test)')
plt.plot(TestLossesToPlotBP,color=sns.color_palette('dark')[0],label='BP (test)')
plt.xlabel('step number')
plt.ylabel('MNIST\n\n loss')
sns.despine()
plt.legend()
plt.ylim(bottom=0)
plt.title('A',loc='left')


plt.subplot(1,4,2)
plt.plot(AccuraciesToPlotPC,color=sns.color_palette('pastel')[3],label='Modified PC (train)')
plt.plot(AccuraciesToPlotBP,color=sns.color_palette('pastel')[0],label='BP (train)')
plt.plot(TestAccuraciesToPlotPC,color=sns.color_palette('dark')[3],label='Modified PC (test)')
plt.plot(TestAccuraciesToPlotBP,color=sns.color_palette('dark')[0],label='BP (test)')
plt.xlabel('step number')
plt.ylabel('accuracy')
sns.despine()
#plt.legend()
plt.ylim(bottom=0)
plt.title('B',loc='left')

plt.subplot(1,4,3)
for layer in range(len(model)):
  plt.plot(GradsRelDiff0[:,layer],label='layer '+str(layer+1))
#plt.plot(GradsRelDiff)
plt.xlabel('step number')
plt.ylabel(r'relative error of d$\theta$'+'\nfrom gradient')
plt.legend(loc='center')
sns.despine()
plt.title('C',loc='left')

plt.subplot(1,4,4)
plt.plot((180/3.14159)*GradsAngle0)
plt.xlabel('step number')
plt.ylabel(r'angle between d$\theta$ and'+'\n gradient (degrees)')
sns.despine()
plt.title('D',loc='left')

plt.tight_layout()

if SaveFigures:
  plt.savefig('/content/gdrive/MyDrive/PredictiveCodingANN/Figures/PCMNIST.pdf')

"""Exp 2"""

batch_size = 300      # Batch size to use with training data
test_batch_size = 300 # Batch size to use for test data

# Data loader. These make it easy to iterate through batches of data.
# Shuffle=True means that the data will be randomly shuffled on every epoch
train_loader = torch.utils.data.DataLoader(dataset=train_dataset,
                                          batch_size=batch_size,
                                          shuffle=True)
test_loader = torch.utils.data.DataLoader(dataset=test_dataset,
                                          batch_size=test_batch_size,
                                          shuffle=True)


steps_per_epoch = len(train_loader) # = mini batch size = m'
print("steps per epoch (mini batch size)=",steps_per_epoch)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('device = ',device)

model=nn.Sequential(

    nn.Sequential(nn.Conv2d(1,10,3),
    nn.ReLU(),
    nn.MaxPool2d(2)
    ),

    nn.Sequential(
    nn.Conv2d(10,5,3),
    nn.ReLU(),
    nn.Flatten()
    ),

 nn.Sequential(
    nn.Linear(5*11*11,50),
    nn.ReLU()
    ),

 nn.Sequential(
    nn.Linear(50,30),
    nn.ReLU()
    ),

 nn.Sequential(
    nn.Linear(30, 20),  # New additional layer
    nn.ReLU()
    ),


nn.Sequential(
   nn.Linear(20,10)
 )

).to(device)

LossFun = nn.MSELoss()

# Compute one output and one loss to make sure
# things are working
with torch.no_grad():
  TrainingIterator=iter(train_loader)
  X,Y=next(TrainingIterator)
  X=X.to(device)
  Y=ToOneHot(Y,10).to(device)
  Yhat=model(X).to(device)
  print('output shape = ',Yhat.shape)
  print('loss on initial model = ',LossFun(Yhat,Y).item())

NumParams=sum(p.numel() for p in model.parameters() if p.requires_grad)
print('Number of trainable parameters in model =',NumParams)


LearningRate=.002
num_epochs=2
ComputeTrainingMetrics=True
WhichOptimizer=torch.optim.Adam
PrintEvery=50
total_num_steps  = num_epochs*steps_per_epoch

torch.manual_seed(0)

modelPC=deepcopy(model)


optimizerPC = WhichOptimizer(modelPC.parameters(), lr=LearningRate)


jj=0
LossesToPlotPC=np.zeros(total_num_steps)
TestLossesToPlotPC=np.zeros(total_num_steps)
AccuraciesToPlotPC=np.zeros(total_num_steps)
TestAccuraciesToPlotPC=np.zeros(total_num_steps)

GradsRelDiff0=np.zeros([total_num_steps,len(model)])
GradsCosSim0=np.zeros([total_num_steps,len(model)])
GradsAngle0=np.zeros([total_num_steps,len(model)])

def RelDiff(x,y):
  return np.linalg.norm(x-y)/np.linalg.norm(y)

def corr2(x,y):
  c=np.corrcoef(x,y)
  return c[0,1]

CosSim = nn.CosineSimilarity(dim=0, eps=1e-8)


eta=.1
n=20

j=0     # Counter to keep track of iterations
t1=tm() # Start the timer

for k in range(num_epochs):

  # Re-initializes the training iterator (shuffles data for one epoch)
  TrainingIterator=iter(train_loader)

  for i in range(steps_per_epoch): # For each batch

    # Get one batch of training data, reshape it
    # and send it to the current device
    X,Y=next(TrainingIterator)
    X=X.to(device)
    Y=ToOneHot(Y,10).to(device)

    _,Loss,_,_,_=T2PC.PCInfer(modelPC,LossFun,X,Y,"Strict",eta,n)

    if ComputeTrainingMetrics:
      modelBP=deepcopy(modelPC)   # Copy the model
      YhatBP = modelBP(X)         # Forward pass
      LossBP = LossFun(YhatBP, Y)
      LossBP.backward()       # Compute gradients
      for layer in range(len(model)):
        gradsPC=modelPC[layer][0].weight.grad.cpu().detach().numpy()
        gradsBP=modelBP[layer][0].weight.grad.cpu().detach().numpy()
        GradsRelDiff0[jj,layer]=RelDiff(gradsPC,gradsBP)
        GradsCosSim0[jj,layer]=CosSim(torch.tensor(gradsPC.flatten()),torch.tensor(gradsBP.flatten())).item()
        GradsAngle0[jj,layer]=torch.acos(torch.tensor(GradsCosSim0[jj,layer])).item()
      modelBP.zero_grad()

    # Update parameters
    optimizerPC.step()


    # Zero-out gradients
    modelPC.zero_grad()
    optimizerPC.zero_grad()

    # Print loss, store loss, compute test loss
    with torch.no_grad():
      if(i%PrintEvery==0):
        print('k =',k,'i =',i,'L =',Loss.item())
      LossesToPlotPC[jj]=Loss.item()

      if ComputeTrainingMetrics:
        Yhat=modelPC(X)
        AccuraciesToPlotPC[jj]=(torch.sum(torch.argmax(Y,axis=1)==torch.argmax(Yhat,axis=1))/test_batch_size).item()
        modelPC.eval()
        TestingIterator=iter(test_loader)
        Xtest,Ytest=next(TestingIterator)
        Xtest=Xtest.to(device)
        Ytest=ToOneHot(Ytest,10).to(device)
        YhatTest=modelPC(Xtest)
        TestLossesToPlotPC[jj]=LossFun(YhatTest,Ytest).item()
        TestAccuraciesToPlotPC[jj]=(torch.sum(torch.argmax(Ytest,axis=1)==torch.argmax(YhatTest,axis=1))/test_batch_size).item()
        modelPC.train()
      jj+=1




tTrain=tm()-t1
print('Training time = ',tTrain,'sec')

plt.figure()
plt.plot(LossesToPlotPC)
plt.ylim(bottom=0)
plt.ylabel('training loss')

plt.figure()
plt.plot(GradsRelDiff0)
plt.ylabel('rel err of dtheta')

torch.manual_seed(0)

modelBP=deepcopy(model)

optimizerBP = WhichOptimizer(modelBP.parameters(), lr=LearningRate)

total_num_steps  = num_epochs*steps_per_epoch


jj=0
LossesToPlotBP=np.zeros(total_num_steps)
TestLossesToPlotBP=np.zeros(total_num_steps)
AccuraciesToPlotBP=np.zeros(total_num_steps)
TestAccuraciesToPlotBP=np.zeros(total_num_steps)


DepthPlusOne=len(modelBP)+1 # Number of layers, counting the input as layer 0

j=0     # Counter to keep track of iterations
t1=tm() # Start the timer
for k in range(num_epochs):

  # Re-initializes the training iterator (shuffles data for one epoch)
  TrainingIterator=iter(train_loader)

  for i in range(steps_per_epoch): # For each batch

    # Get one batch of training data, reshape it
    # and send it to the current device
    X,Y=next(TrainingIterator)
    X=X.to(device)
    Y=ToOneHot(Y,10).to(device)


    Yhat = modelBP(X)     # Forward pass
    Loss = LossFun(Yhat, Y)
    Loss.backward()       # Compute gradients
    optimizerBP.step()      # Update parameters


    # Zero-out gradients
    modelBP.zero_grad()
    optimizerBP.zero_grad()

    with torch.no_grad():
      if(i%PrintEvery==0):
        print('k =',k,'i =',i,'L =',Loss.item())
      LossesToPlotBP[jj]=Loss.item()
      if ComputeTrainingMetrics:
        AccuraciesToPlotBP[jj]=(torch.sum(torch.argmax(Y,axis=1)==torch.argmax(Yhat,axis=1))/test_batch_size).item()
        modelBP.eval()
        TestingIterator=iter(test_loader)
        Xtest,Ytest=next(TestingIterator)
        Xtest=Xtest.to(device)
        Ytest=ToOneHot(Ytest,10).to(device)
        YhatTest=modelBP(Xtest)
        TestLossesToPlotBP[jj]=LossFun(YhatTest,Ytest).item()
        TestAccuraciesToPlotBP[jj]=(torch.sum(torch.argmax(Ytest,axis=1)==torch.argmax(YhatTest,axis=1))/test_batch_size).item()
        modelBP.train()
      jj+=1

tTrainBP=tm()-t1
print('Training time = ',tTrainBP,'sec')

plt.figure()
plt.plot(LossesToPlotBP)
plt.ylim(bottom=0)
plt.ylabel('training loss')

sns.set(context='notebook',style='white',font_scale=1.3)

fig, axes = plt.subplots(figsize=(17, 3))


plt.subplot(1,4,1)
plt.plot(LossesToPlotPC,color=sns.color_palette('pastel')[3],label='Modified PC (train)')
plt.plot(LossesToPlotBP,color=sns.color_palette('pastel')[0],label='BP (train)')
plt.plot(TestLossesToPlotPC,color=sns.color_palette('dark')[3],label='Modified PC (test)')
plt.plot(TestLossesToPlotBP,color=sns.color_palette('dark')[0],label='BP (test)')
plt.xlabel('step number')
plt.ylabel('MNIST\n\n loss')
sns.despine()
plt.legend()
plt.ylim(bottom=0)
plt.title('A',loc='left')


plt.subplot(1,4,2)
plt.plot(AccuraciesToPlotPC,color=sns.color_palette('pastel')[3],label='Modified PC (train)')
plt.plot(AccuraciesToPlotBP,color=sns.color_palette('pastel')[0],label='BP (train)')
plt.plot(TestAccuraciesToPlotPC,color=sns.color_palette('dark')[3],label='Modified PC (test)')
plt.plot(TestAccuraciesToPlotBP,color=sns.color_palette('dark')[0],label='BP (test)')
plt.xlabel('step number')
plt.ylabel('accuracy')
sns.despine()
#plt.legend()
plt.ylim(bottom=0)
plt.title('B',loc='left')

plt.subplot(1,4,3)
for layer in range(len(model)):
  plt.plot(GradsRelDiff0[:,layer],label='layer '+str(layer+1))
#plt.plot(GradsRelDiff)
plt.xlabel('step number')
plt.ylabel(r'relative error of d$\theta$'+'\nfrom gradient')
plt.legend(loc='center')
sns.despine()
plt.title('C',loc='left')

plt.subplot(1,4,4)
plt.plot((180/3.14159)*GradsAngle0)
plt.xlabel('step number')
plt.ylabel(r'angle between d$\theta$ and'+'\n gradient (degrees)')
sns.despine()
plt.title('D',loc='left')

plt.tight_layout()

if SaveFigures:
  plt.savefig('/content/gdrive/MyDrive/PredictiveCodingANN/Figures/PCMNIST.pdf')